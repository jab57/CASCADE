# =============================================================================
# CASCADE MCP Server Configuration
# =============================================================================
# Copy this file to .env and customize as needed

# =============================================================================
# LLM INSIGHTS CONFIGURATION (Optional)
# =============================================================================
# Enable LLM-powered biological insights in comprehensive_perturbation_analysis
# Set to 'true' to enable (requires Ollama running locally or cloud API key)
USE_LLM_INSIGHTS=false

# -----------------------------------------------------------------------------
# Ollama Local Mode (DEFAULT - no API key needed)
# -----------------------------------------------------------------------------
# Install Ollama: https://ollama.ai
# Pull a model: ollama pull llama3.1:8b
# Start server: ollama serve (runs on localhost:11434 by default)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# -----------------------------------------------------------------------------
# Ollama Cloud Mode (set API key to enable)
# -----------------------------------------------------------------------------
# Get API key from https://ollama.com
# When set, cloud mode is used instead of local
# OLLAMA_API_KEY=your-ollama-cloud-key-here

# -----------------------------------------------------------------------------
# LLM Performance Tuning
# -----------------------------------------------------------------------------
# Timeout for LLM calls in seconds (default: 60)
OLLAMA_TIMEOUT=60

# Temperature for LLM generation (0.0-1.0, lower = more deterministic)
OLLAMA_TEMPERATURE=0.3

# Maximum tokens for LLM response
OLLAMA_MAX_TOKENS=2000
